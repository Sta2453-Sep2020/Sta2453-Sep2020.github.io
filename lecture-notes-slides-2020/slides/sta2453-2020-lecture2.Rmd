---
title: 'Lecture 2: Linear Regression'
output:
  html_document:
    df_print: paged
---

```{r global_options, include=FALSE, cache=FALSE}
library(knitr)

opts_chunk$set(echo=TRUE, 
               warning=FALSE, 
               message=FALSE,
               cache = FALSE,
               include = TRUE,
               error = TRUE)
               
               
# setup changes according to html or docx
output <- opts_knit$get("rmarkdown.pandoc.to")
if (output=="html") {
  
  opts_chunk$set(fig.width=11, 
                 fig.height=11)
  
  } # # end html `if` statement
## setting up the figure parameters for docx
if (output=="docx") {
    opts_chunk$set(dev = 'pdf',
                   fig.width = 6, 
                   fig.height = 6)
                     } # end docx `if` statement
```


![](bikeshare1.png)   

# Open Toronto Data


## About Bike Share Toronto Ridership Data


The [Bike Share Toronto Ridership data](https://open.toronto.ca/dataset/bike-share-toronto-ridership-data/) (2016-onwards) contains anonymized trip data, including:

- Trip start day and time
- Trip end day and time
- Trip duration
- Trip start station
- Trip end station
- User type


```{r load_data}

# Load the required packages

# tidyverse for ingesting/cleaning/plotting data
library(tidyverse)

# lubridate for working with dates/timestamps
library(lubridate)

# rvest for scraping web data
library(rvest)

# read in the data
bike <- read_csv('Bikeshare Ridership (2017 Q1).csv')

# print a table of the first 6 observations
DT::datatable(head(bike), options = list(scrollX = T))

```


## Question: 

**Is the duration of a bike influenced by weather? For example, do Bike Share customers take longer bike rides in warmer weather?**


### Initial data exploration

```{r exploration}

# number of observations and variables
cat("number of rows:", nrow(bike), "Number of columns:", ncol(bike), "\n")

# or we could print both at once
dim(bike)

# a look at the data types
glimpse(bike)

# is the trip_id unique 
n_distinct(bike$trip_id) == nrow(bike)

# dates are character variables

# what is the format?

time_splits <- strsplit(bike$trip_start_time, "/")

# the first looks like day
table(as.numeric(sapply(time_splits, "[[", 1)))

# the second looks like month
table(as.numeric(sapply(time_splits, "[[", 2)))

# split the last section by the space
sub_time_split <- strsplit(sapply(time_splits, "[[", 3), " ")

# the third looks like year
table(as.numeric(sapply(sub_time_split, "[[", 1)))

# split the remaining section by :

sub_time_split2 <- strsplit(sapply(sub_time_split, "[[", 2), ":")

# the fourth looks like hour
table(as.numeric(sapply(sub_time_split2, "[[", 1)))

# the fifth looks like minute
table(as.numeric(sapply(sub_time_split2, "[[", 2)))

# so the format is day/month/year hour:minute (we can use lubridate)

bike <- bike %>% 
  mutate(trip_start_time = lubridate::dmy_hm(trip_start_time, tz = "America/New_York"),
         trip_stop_time = lubridate::dmy_hm(trip_stop_time, tz = "America/New_York"))

bike %>% 
  count(hour = lubridate::hour(trip_start_time)) %>% 
  ggplot(aes(hour, n)) +
  geom_bar(stat = "identity") +
  xlab("hour of trip start") +
  ylab("count") +
  ggtitle("Distribution of trip start time hour") +
  scale_x_continuous(breaks = 1:23)


```



```{r}

# convert timestamps to easter
bike <- bike %>% 
  mutate(trip_start_ts = lubridate::with_tz(trip_start_time, tzone = "EST"),
         trip_stop_ts = lubridate::with_tz(trip_stop_time, tzone = "EST"))

# that's what we expect!
bike %>% 
  count(hour = lubridate::hour(trip_start_ts)) %>% 
  ggplot(aes(hour, n)) +
  geom_bar(stat = "identity") +
  xlab("hour of trip start") +
  ylab("count") +
  ggtitle("Distribution of trip start time hour") +
  scale_x_continuous(breaks =1:23)


# day of month

bike %>% 
  count(day = lubridate::day(trip_start_ts)) %>% 
  ggplot(aes(day, n)) +
  geom_bar(stat = "identity") +
  xlab("day of month of trip start") +
  ylab("count") +
  ggtitle("Distribution of trip start time",
          subtitle = "day of month") +
  scale_x_continuous(breaks =1:31)


# Month plot

bike %>% 
  count(month = as_factor(lubridate::month(trip_start_ts))) %>% 
  ggplot(aes(month, n)) +
  geom_bar(stat = "identity") +
  xlab("month of trip start") +
  ylab("count") +
  ggtitle("Distribution of trip start time",
          subtitle = "by month") 

# avg number of trips by day of week

bike %>% 
  count(date = as_date(trip_start_ts)) %>% 
  mutate(day_of_week = wday(date, label = T)) %>% 
  ggplot(aes(day_of_week, n)) +
  geom_boxplot() +
  xlab("Day of week") +
  ylab("Average trips by day of week") +
  ggtitle("Distribution of trips",
          subtitle = "by day of week") 


```



```{r}

## Create some additional features

bike <- bike %>% 
  mutate_at(vars("trip_start_ts"), .funs = c(start_month = lubridate::month,
                                             start_day_of_week = lubridate::wday,
                                             start_day = lubridate::day,
                                             start_hr = lubridate::hour,
                                             start_min = lubridate::minute)) %>% 
  mutate(date = as_date(trip_start_ts))

## Remove late 2016 values
bike <- bike %>% filter(year(date) == 2017)

```



## Adding in weather data

```{r}


clean_var_names <- function(x) {
  
  clean_x <- tolower(x) %>% 
    str_replace_all(., "/|\'|Â°", "") %>% 
    str_replace_all(., " ", "_")
  
  return(clean_x)
}




scrape_weather <- function(station = "51459", 
                           year = 2017, 
                           month = 3) {
  
  query_url <- glue::glue('http://climate.weather.gc.ca/climate_data/daily_data_e.html?StationID={station}&timeframe=2&Year={year}&Month={month}#')
  
  weather_data <- query_url %>% 
    read_html() %>% 
    html_table() %>% 
    .[[1]] %>% 
    as_tibble()
  
  
  names(weather_data) <- clean_var_names(names(weather_data))
  
  weather_data <- weather_data %>% 
    mutate(date = ymd(paste(year, month, day, '-'))) %>% 
    select(date, mean_temp_definitionc, total_precip_definitionmm) %>% 
    filter(!is.na(date))
  
  weather_data <- weather_data %>% 
    mutate(total_precip = as.numeric(ifelse(total_precip_definitionmm == "LegendTT", 
                                            "0.01",total_precip_definitionmm)),
           total_precip = ifelse(is.na(total_precip), 0, total_precip),
           mean_temp = as.numeric(mean_temp_definitionc)) %>% 
    select(date, mean_temp, total_precip)
  
  return(weather_data)
}


```



```{r}
# get weather for jan-march -----------------------------------------------
weather <- list()
months <- 1:3

for(i in months) {
  weather[[i]] <- scrape_weather(month = i)
}

weather <- do.call(rbind, weather)


# get a plot of the weather -----------------------------------------------
weather %>% 
  ggplot(aes(date, mean_temp)) +
  geom_line()



# join bike and weather data ----------------------------------------------

bike <- bike %>% 
  left_join(weather, by = 'date')


```



```{r}
bike_daily <- bike %>% 
  group_by(start_month, start_day, user_type) %>% 
  summarize(trip_duration_seconds = mean(trip_duration_seconds),
            total_precip = mean(total_precip),
            mean_temp = mean(mean_temp)) %>% 
  ungroup()

bike_daily <- bike_daily %>% 
  mutate(log_duration = log(trip_duration_seconds),
         log_precip = log(total_precip + 1))



```

## Linear regression

A simple linear regression model takes the form:

$$
Y = \beta_0 + \beta_1 x + \epsilon
$$
- $\beta_0$ is the intercept term (The average value of Y when X = 0)
- $\beta_1$ is called the slope (The amount Y increases with a 1 unit increase in X)

The error term really contains what we miss with this
simple model:

- The true relationship is probably not linear,
- There may be other variables that cause variation in Y , 
- There may be measurement error. We typically assume that the error term is independent of X


An example linear regression model for our current example is:
  
  $$y_i = \beta_0 + \beta_1 x_1 + \epsilon_i, i = 1,2, \dots, n$$
  

Where $Y_i$ is trip duration, $x_i$ is the temperature, $n$ is the number of observations, and $\epsilon ~ N(0, \sigma^2)$


### Linear Regression Statistical Assumptions


1. For any value of x, y is a $N(\beta_0 + \beta_1 x, \sigma^2)$
2. *Independence of errors*. The simple regression model assumes that the errors
from the prediction line are independent. 
3. *Linearity*: The mean value of $y$ is a linear function of $x$.
4. *Homoscedasticity*: The variance of $y$ is the same for any $x$.


## Estimating regression coefficients

Estimates of the regression coefficients $\beta_0$, $\beta_1$ are obtained by minimizing:

$$\sum_{i=1}^{n}\left( y_i - \left(\beta_0 + \beta_1 x_i\right) \right)^2$$

Using calculus there are closed form expressions for the minimizers: $\hat{\beta_0}$, $\hat{\beta_1}$:

The prediction for y based on the $i^\text{th}$ value of $x$ is:

$$
\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i
$$

$e_i = y_i - \hat{y_i}$, is the $i^\text{th}$ residual, and the residual sum of squares is:

$$
RSS = \sum_{i=1}^{n} e_i^2
$$  

We could rewrite this as

$$
RSS = (y_1 - \hat{\beta_0} -  \hat{\beta_1} x_1)^2 +  (y_2 - \hat{\beta_0} -  \hat{\beta_1} x_2)^2 +\dots +  (y_n - \hat{\beta_0} -  \hat{\beta_1} x_n)^2  
$$

Using a little calculus we can show that:

$$
\hat{\beta_1} = \frac{\sum_{i=1}^{n}\left(x_i -\bar{x} \right)\left(y_i -\bar{y} \right) }{\sum_{i=1}^{n}\left(x_i -\bar{x} \right)^2}
$$

and 

$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
$$


## Accuracy of regression coefficients

The accuracy of the regression coefficients can be evaluated by computing the standard errors of $\hat{\beta_0}$, $\hat{\beta_1}$.

$\sigma^2$ can be estimated by the formula:

$$
\sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}^{n}(y_i - \hat{y_i})^2}
$$

$$
RSE = \frac{RSS}{n - 2}
$$

Standard errors can be used to test:

$$H_0:\text{There is no linear relationship between x and y}$$
$$H_1:\text{There is a linear relationship between x and y}$$

Which corresponds to testing:

$$ H_0: \beta_1 = 0 \text{ vs } H1: \beta_1 \neq 0$$

Tests and confidence intervals are based on:

$$\frac{\hat{\beta_1}}{SE \left(\hat{\beta_1} \right) } \sim t_{n-2}$$

where:

$$
SE(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}
$$

## Accuracy of Model 

The proportion of variability in $y$ that can be explained by linear regression model of $y$ on x.



$$
R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
$$

Where TSS is the true sum of squares: $\sum(y_i-\bar{y})^2$ and RSS is the residual sum of squares: $\sum(y_i - \hat{y_i})^2$






### Example with Simulated data

```{r}

set.seed(2552)

# draw 100 samples from N(5,4)
y <-  rnorm(100, 5, 2) 

x <- y + runif(100, 0,1) 

fake_data <- tibble(x, y)

fake_data %>% 
  ggplot(aes( x, y)) + 
  geom_point()

model <- lm(y ~ x, data = fake_data)

summary(model)
```



A prettier printing method than the raw `summary()` function.

```{r}
library(pander)

pander( summary(model) ) 

```


below is a plot of the line of best fit

```{r}
fake_data %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method='lm', formula= y~x, se = F)


```


## Linear regression with the Bike share data

### Preparing the data for analysis

```{r}

bike_daily <- bike %>% 
  group_by(start_month, start_day, user_type) %>% 
  summarize(trip_duration_seconds = mean(trip_duration_seconds),
            total_precip = mean(total_precip),
            mean_temp = mean(mean_temp))

bike_daily <- bike_daily %>% 
  mutate(log_duration = log(trip_duration_seconds),
         log_precip = log(total_precip + 1))

```


```{r}

model <- lm(trip_duration_seconds~ mean_temp, data = bike_daily)
pander( summary(model) ) 
```

```{r}

bike_daily %>% 
  group_by(user_type) %>% 
  summarize(mean = mean(trip_duration_seconds)) %>% 
  ggplot(aes(user_type, mean)) + geom_bar(stat = 'identity')

```


```{r}

bike_daily %>% 
  ggplot(aes(mean_temp, trip_duration_seconds, color = user_type)) +
  geom_point()

```


### How do we interpret a qualitative predictor/feature?

```{r}

model <- lm(trip_duration_seconds~ user_type, data = bike_daily)
pander( summary(model) ) 

```


$$
x_i =
\begin{cases}
1,& \text{ if the } i^\text{th}\text{ person is a member} \\
0,& \text{ if the } i^\text{th}\text{ person is casual} 

\end{cases}
$$


$$
y_i = \beta_0 + \beta_1 x + \epsilon_i = 
\begin{cases}
\beta_0 + \beta_1 x_i + \epsilon_i  &\text{ if the } i^\text{th}\text{ person is a member} \\
\beta_0 + \epsilon_i  &\text{ if the } i^\text{th}\text{ person is casual} 

\end{cases}
$$

$\beta_0$ is the average duration among members, $\beta_0 + \beta_1$ is the average duration among casual users, and $\beta_1$ is the average difference in duration between members and casual users. Testing $\beta_1$ is equivalent to a two-sample t-test.


```{r}

bike_daily %>% 
  group_by(user_type) %>% 
  summarize(mean_duration = mean(trip_duration_seconds)) %>% 
  mutate(difference = mean_duration - lag(mean_duration))

t.test(bike_daily$trip_duration_seconds ~ bike_daily$user_type)
```


## Adjusting for more than one feature/predictor

### Multiple Linear Regression

Do temperature and user type predict trip duration?

For $p$ potential covariates we have

$$ 

y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \epsilon_i, i = 1\dots n
$$

In matrix form:

$$ 
 Y = X\beta + \epsilon
$$

$Y$ is an $n\times1$ column vector, $X$ is an $n\timesp$ matrix of rank $p$, and $\epsilon$ is an $n\times1$ column vector that has a multivariate normal distribution.



```{r}
model <- lm(trip_duration_seconds~ mean_temp + user_type, data = bike_daily)
pander( summary(model) ) 

```

```{r}

a %>% 
  ggplot(aes(mean_temp, trip_duration_seconds, color = user_type))+
  geom_point() +
  geom_smooth(method = "lm")
  
  
model <- lm(trip_duration_seconds~ mean_temp + user_type + mean_temp*user_type, data = a)
pander( summary(model) ) 

```


## Variable selection

How do we select which variables are entered into the linear regression?

If we have $p$ potential covariates under consideration, then there are $2^p$ potential models under we could consider. Trying to fit all potential submodels isn't feasible, so practitioners have come up with a few algorithms for determining which variables to include. Here are three (there are many more)

- Forward Selection
- Backward Selection
- Mixed selection: This is a combination of the previous 2 algorithms. P-values change as we add variables with forward selection (they get larger). We can remove features whos p-values cross a certain threshold. 


## Linear regression as a machine learning algorithm

1. Select a training and test

2. Fit a linear regression model on the training set.

3. Use the model in 1. to predict the dependent variable in the test set.

4. Calculate a measure of fit on the test and training to evaluate the quality of fit. For example, the mean squared error is:


$$
MSE = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{e^2_i}
$$



```{r}

# regression --------------------------------------------------------------

library(tidymodels)
# resample functions
# Make sure that you get the same random numbers
set.seed(4595)
data_split <- initial_split(bike_daily, prop = .80)

bike_train <- training(data_split)
bike_test  <- testing(data_split)

nrow(bike_train)/nrow(bike_daily)


training(data_split)


# ------------------------------------------------------------------------------
# A Linear Regression Model 

simple_lm <- lm(trip_duration_seconds~ mean_temp + total_precip + user_type, data = bike_train)

simple_lm_values <- augment(simple_lm)
names(simple_lm_values)



# using parsnip -----------------------------------------------------------
spec_lin_reg <- linear_reg()
spec_lin_reg

lm_mod <- set_engine(spec_lin_reg, "lm")
lm_mod

lm_fit <- fit(
  lm_mod,
  trip_duration_seconds~ mean_temp + total_precip + user_type,
  data = bike_train
)


fit_xy(
  lm_mod,
  y = bike_train$trip_duration_seconds,
  x = bike_train %>% dplyr::select(mean_temp, total_precip, user_type)
)


# ------------------------------------------------------------------------------
# Predictions 

# Numeric predictions always in a df
# with column `.pred`

test_pred <- 
  lm_fit %>%
  predict(bike_test) %>%
  bind_cols(bike_test)

test_pred %>% 
  dplyr::select(trip_duration_seconds, .pred) %>% 
  slice(1:3)

# ------------------------------------------------------------------------------
# Estimating Performance 

# yardstick loaded by tidymodels

perf_metrics <- metric_set(rmse, rsq)

# A tidy result back:
test_pred  %>% 
  perf_metrics(truth = trip_duration_seconds, estimate = .pred)




```












